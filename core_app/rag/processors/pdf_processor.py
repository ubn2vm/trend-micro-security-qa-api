"""
PDF è™•ç†æ¨¡çµ„ - å°ˆé–€è™•ç†è¶¨å‹¢ç§‘æŠ€ CREM æŠ€è¡“æ–‡æª”
ç”¨æ–¼æå– PDF æ–‡æœ¬å…§å®¹ä¸¦é€²è¡Œé è™•ç†
"""

import pdfplumber
import re
import logging
import json
from typing import List, Optional, Dict, Any
from pathlib import Path
import os

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CREMPDFProcessor:
    """CREM PDF è™•ç†å™¨ - å°ˆé–€è™•ç†è¶¨å‹¢ç§‘æŠ€æŠ€è¡“æ–‡æª”"""
    
    def __init__(self, pdf_path: str):
        """
        åˆå§‹åŒ– PDF è™•ç†å™¨
        
        Args:
            pdf_path (str): PDF æ–‡ä»¶è·¯å¾‘
        """
        self.pdf_path = Path(pdf_path)
        self.text_content = []
        self.total_pages = 0
        self.extracted_pages = 0
        
        # é©—è­‰æ–‡ä»¶å­˜åœ¨
        if not self.pdf_path.exists():
            raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        
        logger.info(f"åˆå§‹åŒ– PDF è™•ç†å™¨: {pdf_path}")
    
    def extract_text(self) -> str:
        """
        æå– PDF æ–‡æœ¬å…§å®¹
        
        Returns:
            str: æå–çš„æ–‡æœ¬å…§å®¹
        """
        logger.info("é–‹å§‹æå– PDF æ–‡æœ¬...")
        
        try:
            with pdfplumber.open(self.pdf_path) as pdf:
                self.total_pages = len(pdf.pages)
                logger.info(f"PDF ç¸½é æ•¸: {self.total_pages}")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        # æå–é é¢æ–‡æœ¬
                        text = page.extract_text()
                        
                        if text and text.strip():
                            # æ¸…ç†æ–‡æœ¬
                            cleaned_text = self._clean_page_text(text)
                            
                            # æ·»åŠ é ç¢¼æ¨™è¨˜
                            page_marker = f"=== Page {page_num} ===\n"
                            page_content = page_marker + cleaned_text + "\n\n"
                            
                            self.text_content.append(page_content)
                            self.extracted_pages += 1
                            
                            logger.info(f"æˆåŠŸæå–ç¬¬ {page_num} é æ–‡æœ¬ ({len(cleaned_text)} å­—ç¬¦)")
                        else:
                            logger.warning(f"ç¬¬ {page_num} é ç„¡æ–‡æœ¬å…§å®¹")
                            
                    except Exception as e:
                        logger.error(f"æå–ç¬¬ {page_num} é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                        continue
            
            # åˆä½µæ‰€æœ‰é é¢æ–‡æœ¬
            full_text = ''.join(self.text_content)
            
            logger.info(f"æ–‡æœ¬æå–å®Œæˆ: {self.extracted_pages}/{self.total_pages} é æˆåŠŸ")
            logger.info(f"ç¸½æ–‡æœ¬é•·åº¦: {len(full_text)} å­—ç¬¦")
            
            return full_text
            
        except Exception as e:
            logger.error(f"PDF æ–‡æœ¬æå–å¤±æ•—: {str(e)}")
            raise
    
    def _clean_page_text(self, text: str) -> str:
        """
        æ¸…ç†é é¢æ–‡æœ¬
        
        Args:
            text (str): åŸå§‹é é¢æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†å¾Œçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ä¿®å¾©é‡è¤‡å­—ç¬¦å•é¡Œ (å¦‚ "SSoolluuttiioonn" -> "Solution")
        text = self._fix_duplicate_characters(text)
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤é çœ‰é è…³ç­‰å¸¸è¦‹å™ªéŸ³
        text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)  # ç§»é™¤å–®ç¨çš„é ç¢¼
        
        # ä¿æŒæŠ€è¡“è¡“èªçš„å®Œæ•´æ€§
        # ä¿è­· CREMã€CRIã€AIã€ML ç­‰æŠ€è¡“è¡“èª
        text = re.sub(r'\b(CREM|CRI|AI|ML|XDR|EDR|SAE)\b', r' \1 ', text, flags=re.IGNORECASE)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™é‡è¦æ¨™é»
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\{\}\"\']', '', text)
        
        # ä¿®å¾©å¸¸è¦‹çš„æ–‡æœ¬å•é¡Œ
        text = re.sub(r'(\w)\.(\w)', r'\1. \2', text)  # ä¿®å¾©å¥è™Ÿå¾Œç¼ºå°‘ç©ºæ ¼
        text = re.sub(r'(\w)\,(\w)', r'\1, \2', text)  # ä¿®å¾©é€—è™Ÿå¾Œç¼ºå°‘ç©ºæ ¼
        
        return text.strip()
    
    def _fix_duplicate_characters(self, text: str) -> str:
        """
        ä¿®å¾©é‡è¤‡å­—ç¬¦å•é¡Œ
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: ä¿®å¾©å¾Œçš„æ–‡æœ¬
        """
        # ä¿®å¾©å¸¸è¦‹çš„é‡è¤‡å­—ç¬¦æ¨¡å¼
        duplicate_patterns = [
            (r'([A-Z])\1+', r'\1'),  # ä¿®å¾©å¤§å¯«å­—æ¯é‡è¤‡
            (r'([a-z])\1+', r'\1'),  # ä¿®å¾©å°å¯«å­—æ¯é‡è¤‡
            (r'([0-9])\1+', r'\1'),  # ä¿®å¾©æ•¸å­—é‡è¤‡
        ]
        
        for pattern, replacement in duplicate_patterns:
            text = re.sub(pattern, replacement, text)
        
        # ä¿®å¾©ç‰¹å®šçš„æŠ€è¡“è¡“èª
        term_fixes = {
            'SSoolluuttiioonn': 'Solution',
            'BBrriieeff': 'Brief',
            'TREND VISION ONETM': 'TREND VISION ONE',
            'CCyymmeerr RRiisskk': 'Cyber Risk',
            'EExxppoossuurree': 'Exposure',
            'MMaannaaggeemmeenntt': 'Management',
            'CCRREEM': 'CREM',
            'PPrrooaaccttiivveellyy': 'Proactively',
            'uunnccoovveerr': 'uncover',
            'pprreeddiicctt': 'predict',
            'aasssseessss': 'assess',
            'mmiittiiggaattee': 'mitigate',
            'ccyymmeerr': 'cyber',
            'rriisskkss': 'risks',
            'pprriioorriittiizziinngg': 'prioritizing',
            'iimmppaacctt': 'impact',
            'rreedduucciinngg': 'reducing',
            'eexxppoossuurree': 'exposure',
            'bbuuiillddiinngg': 'building',
            'rreessiilliieennccee': 'resilience',
        }
        
        for wrong, correct in term_fixes.items():
            text = text.replace(wrong, correct)
        
        return text
    
    def get_extraction_stats(self) -> dict:
        """
        ç²å–æå–çµ±è¨ˆè³‡è¨Š
        
        Returns:
            dict: æå–çµ±è¨ˆè³‡è¨Š
        """
        return {
            "total_pages": self.total_pages,
            "extracted_pages": self.extracted_pages,
            "success_rate": (self.extracted_pages / self.total_pages * 100) if self.total_pages > 0 else 0,
            "total_text_length": sum(len(text) for text in self.text_content)
        }
    
    def validate_text_quality(self, text: str) -> dict:
        """
        é©—è­‰æ–‡æœ¬å“è³ª
        
        Args:
            text (str): è¦é©—è­‰çš„æ–‡æœ¬
            
        Returns:
            dict: å“è³ªè©•ä¼°çµæœ
        """
        if not text:
            return {"quality_score": 0, "issues": ["æ–‡æœ¬ç‚ºç©º"]}
        
        issues = []
        score = 100
        
        # æª¢æŸ¥æ–‡æœ¬é•·åº¦
        if len(text) < 100:
            issues.append("æ–‡æœ¬éçŸ­")
            score -= 20
        
        # æª¢æŸ¥æŠ€è¡“è¡“èª
        crem_terms = ["CREM", "CRI", "Cyber Risk", "Risk Management", "AI", "Machine Learning"]
        found_terms = sum(1 for term in crem_terms if term.lower() in text.lower())
        
        if found_terms < 2:
            issues.append("ç¼ºå°‘é—œéµæŠ€è¡“è¡“èª")
            score -= 15
        
        # æª¢æŸ¥æ–‡æœ¬çµæ§‹
        sentences = text.split('.')
        if len(sentences) < 5:
            issues.append("å¥å­æ•¸é‡éå°‘")
            score -= 10
        
        # æª¢æŸ¥ç·¨ç¢¼å•é¡Œ
        if '?' in text or '' in text:
            issues.append("å¯èƒ½å­˜åœ¨ç·¨ç¢¼å•é¡Œ")
            score -= 10
        
        return {
            "quality_score": max(0, score),
            "issues": issues,
            "found_technical_terms": found_terms,
            "sentence_count": len(sentences),
            "text_length": len(text)
        }
    
    def save_extraction_result(self, output_path: str, text: str) -> None:
        """
        ä¿å­˜æå–çµæœ
        
        Args:
            output_path (str): è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
            text (str): æå–çš„æ–‡æœ¬
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(text)
            logger.info(f"æå–çµæœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æå–çµæœå¤±æ•—: {str(e)}")
            raise

def extract_pdf_text(pdf_path: str) -> str:
    """
    ä¾¿æ·å‡½æ•¸ï¼šæå– PDF æ–‡æœ¬
    
    Args:
        pdf_path (str): PDF æ–‡ä»¶è·¯å¾‘
        
    Returns:
        str: æå–çš„æ–‡æœ¬å…§å®¹
    """
    processor = CREMPDFProcessor(pdf_path)
    return processor.extract_text()

def comprehensive_validation(pdf_path: str, output_dir: str = None) -> Dict[str, Any]:
    """
    ç¶œåˆé©—è­‰ PDF æå–åŠŸèƒ½
    
    Args:
        pdf_path (str): PDF æ–‡ä»¶è·¯å¾‘
        output_dir (str): è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼‰
        
    Returns:
        dict: å®Œæ•´çš„é©—è­‰çµæœ
    """
    try:
        # 1. åˆå§‹åŒ–è™•ç†å™¨
        processor = CREMPDFProcessor(pdf_path)
        
        # 2. æå–æ–‡æœ¬
        text = processor.extract_text()
        
        # 3. ç²å–çµ±è¨ˆè³‡è¨Š
        stats = processor.get_extraction_stats()
        
        # 4. é©—è­‰æ–‡æœ¬å“è³ª
        quality = processor.validate_text_quality(text)
        
        # 5. ä¿å­˜çµæœï¼ˆå¦‚æœæŒ‡å®šäº†è¼¸å‡ºç›®éŒ„ï¼‰
        if output_dir:
            output_path = Path(output_dir) / "extracted_text.txt"
            processor.save_extraction_result(str(output_path), text)
            
            # ä¿å­˜é©—è­‰å ±å‘Š
            report_path = Path(output_dir) / "validation_report.json"
            report = {
                "extraction_stats": stats,
                "quality_assessment": quality,
                "validation_timestamp": str(Path().cwd()),
                "pdf_path": str(pdf_path)
            }
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
        
        # 6. è¿”å›å®Œæ•´çµæœ
        return {
            "success": True,
            "extraction_stats": stats,
            "quality_assessment": quality,
            "sample_text": text[:1000] + "..." if len(text) > 1000 else text,
            "technical_terms_found": quality.get("found_technical_terms", 0),
            "quality_score": quality.get("quality_score", 0)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "extraction_stats": {},
            "quality_assessment": {}
        }

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    pdf_path = "data/sb-crem.pdf"
    output_dir = "data"
    
    logger.info("=== PDF æ–‡æœ¬æå–ç¶œåˆé©—è­‰ ===")
    result = comprehensive_validation(pdf_path, output_dir)
    
    if result["success"]:
        logger.info("âœ… æå–æˆåŠŸ")
        logger.info(f"ğŸ“Š çµ±è¨ˆè³‡è¨Š: {result['extraction_stats']}")
        logger.info(f"ğŸ“ˆ å“è³ªè©•ä¼°: {result['quality_assessment']}")
        logger.info(f"ğŸ” æŠ€è¡“è¡“èªæ•¸é‡: {result['technical_terms_found']}")
        logger.info(f"â­ å“è³ªåˆ†æ•¸: {result['quality_score']}/100")
        logger.info(f"ğŸ“ æ–‡æœ¬æ¨£æœ¬:\n{result['sample_text'][:500]}...")
        
        # é©—è­‰æ¨™æº–æª¢æŸ¥
        logger.info("\n=== é©—è­‰æ¨™æº–æª¢æŸ¥ ===")
        stats = result['extraction_stats']
        quality = result['quality_assessment']
        
        checks = [
            ("PDF æ–‡æœ¬æå–æˆåŠŸç‡ > 95%", stats.get('success_rate', 0) >= 95),
            ("æ–‡æœ¬é•·åº¦ > 1000 å­—ç¬¦", quality.get('text_length', 0) > 1000),
            ("å“è³ªåˆ†æ•¸ > 80", quality.get('quality_score', 0) > 80),
            ("æŠ€è¡“è¡“èªæ•¸é‡ >= 2", quality.get('found_technical_terms', 0) >= 2),
            ("å¥å­æ•¸é‡ >= 5", quality.get('sentence_count', 0) >= 5)
        ]
        
        for check_name, passed in checks:
            status = "âœ…" if passed else "âŒ"
            logger.info(f"{status} {check_name}")
        
        all_passed = all(passed for _, passed in checks)
        logger.info(f"\nğŸ¯ æ•´é«”é©—è­‰çµæœ: {'é€šé' if all_passed else 'éœ€è¦æ”¹é€²'}")
        
    else:
        logger.error(f"âŒ æå–å¤±æ•—: {result['error']}") 