"""
æ–‡æœ¬è™•ç†æ¨¡çµ„ - å°ˆé–€è™•ç† CREM æŠ€è¡“æ–‡æª”çš„æ¸…ç†èˆ‡åˆ†å¡Š
ç”¨æ–¼æ–‡æœ¬æ¸…ç†ã€æ™ºèƒ½åˆ†å¡Šå’Œå“è³ªé©—è­‰
"""

import re
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CREMTextProcessor:
    """CREM æ–‡æœ¬è™•ç†å™¨ - å°ˆé–€è™•ç†è¶¨å‹¢ç§‘æŠ€æŠ€è¡“æ–‡æª”"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æœ¬è™•ç†å™¨"""
        self.technical_terms = [
            "CREM", "CRI", "Cyber Risk", "Risk Management", "AI", "Machine Learning",
            "XDR", "EDR", "SAE", "Trend Vision One", "Exposure Management",
            "Threat Detection", "Security Operations", "SOC", "Incident Response",
            "Vulnerability Management", "Compliance", "Governance", "Automation"
        ]
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        logger.info("åˆå§‹åŒ– CREM æ–‡æœ¬è™•ç†å™¨")
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬å…§å®¹
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†å¾Œçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        logger.info("é–‹å§‹æ¸…ç†æ–‡æœ¬...")
        
        # 1. ç§»é™¤é ç¢¼æ¨™è¨˜
        text = re.sub(r'=== Page \d+ ===\n', '', text)
        
        # 2. ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™é‡è¦æ¨™é»
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\{\}\"\']', '', text)
        
        # 4. ä¿è­·æŠ€è¡“è¡“èª
        text = self._protect_technical_terms(text)
        
        # 5. ä¿®å¾©å¸¸è¦‹çš„æ–‡æœ¬å•é¡Œ
        text = self._fix_common_issues(text)
        
        # 6. ç§»é™¤é‡è¤‡å…§å®¹
        text = self._remove_duplicates(text)
        
        # 7. æ¨™æº–åŒ–æ ¼å¼
        text = self._normalize_format(text)
        
        logger.info(f"æ–‡æœ¬æ¸…ç†å®Œæˆï¼Œé•·åº¦: {len(text)} å­—ç¬¦")
        return text.strip()
    
    def _protect_technical_terms(self, text: str) -> str:
        """
        ä¿è­·æŠ€è¡“è¡“èªä¸è¢«æ¸…ç†æ‰
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: ä¿è­·å¾Œçš„æ–‡æœ¬
        """
        # ç‚ºæŠ€è¡“è¡“èªæ·»åŠ ç‰¹æ®Šæ¨™è¨˜
        for term in self.technical_terms:
            if term.lower() in text.lower():
                # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼é€²è¡Œå¤§å°å¯«ä¸æ•æ„Ÿçš„æ›¿æ›
                pattern = re.compile(re.escape(term), re.IGNORECASE)
                text = pattern.sub(f"__{term.upper()}__", text)
        
        return text
    
    def _fix_common_issues(self, text: str) -> str:
        """
        ä¿®å¾©å¸¸è¦‹çš„æ–‡æœ¬å•é¡Œ
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: ä¿®å¾©å¾Œçš„æ–‡æœ¬
        """
        # ä¿®å¾©å¥è™Ÿå¾Œç¼ºå°‘ç©ºæ ¼
        text = re.sub(r'(\w)\.(\w)', r'\1. \2', text)
        
        # ä¿®å¾©é€—è™Ÿå¾Œç¼ºå°‘ç©ºæ ¼
        text = re.sub(r'(\w)\,(\w)', r'\1, \2', text)
        
        # ä¿®å¾©å†’è™Ÿå¾Œç¼ºå°‘ç©ºæ ¼
        text = re.sub(r'(\w)\:(\w)', r'\1: \2', text)
        
        # ä¿®å¾©åˆ†è™Ÿå¾Œç¼ºå°‘ç©ºæ ¼
        text = re.sub(r'(\w)\;(\w)', r'\1; \2', text)
        
        # ç§»é™¤å¤šé¤˜çš„æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'[\.\,\;\:\!\?]{2,}', '.', text)
        
        return text
    
    def _remove_duplicates(self, text: str) -> str:
        """
        ç§»é™¤é‡è¤‡å…§å®¹
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: å»é‡å¾Œçš„æ–‡æœ¬
        """
        # ç§»é™¤é€£çºŒçš„ç›¸åŒå¥å­
        sentences = text.split('.')
        unique_sentences = []
        prev_sentence = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence != prev_sentence:
                unique_sentences.append(sentence)
                prev_sentence = sentence
        
        return '. '.join(unique_sentences)
    
    def _normalize_format(self, text: str) -> str:
        """
        æ¨™æº–åŒ–æ–‡æœ¬æ ¼å¼
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¨™æº–åŒ–å¾Œçš„æ–‡æœ¬
        """
        # æ¢å¾©æŠ€è¡“è¡“èªæ¨™è¨˜
        for term in self.technical_terms:
            text = text.replace(f"__{term.upper()}__", term)
        
        # ç¢ºä¿å¥å­ä»¥å¥è™Ÿçµå°¾
        if text and not text.endswith('.'):
            text += '.'
        
        return text
    
    def chunk_text(self, text: str) -> List[Document]:
        """
        æ™ºèƒ½åˆ†å¡Šæ–‡æœ¬
        
        Args:
            text (str): æ¸…ç†å¾Œçš„æ–‡æœ¬
            
        Returns:
            List[Document]: åˆ†å¡Šå¾Œçš„æ–‡æª”åˆ—è¡¨
        """
        if not text:
            return []
        
        logger.info("é–‹å§‹æ–‡æœ¬åˆ†å¡Š...")
        
        try:
            # ä½¿ç”¨ LangChain çš„æ–‡æœ¬åˆ†å‰²å™¨
            chunks = self.text_splitter.split_text(text)
            
            # è½‰æ›ç‚º Document å°è±¡
            documents = []
            for i, chunk in enumerate(chunks):
                if chunk.strip():  # åªä¿ç•™éç©ºå¡Š
                    doc = Document(
                        page_content=chunk.strip(),
                        metadata={
                            "chunk_id": i,
                            "source": "sb-crem.pdf",
                            "chunk_size": len(chunk),
                            "technical_terms": self._extract_technical_terms(chunk)
                        }
                    )
                    documents.append(doc)
            
            logger.info(f"æ–‡æœ¬åˆ†å¡Šå®Œæˆï¼Œå…± {len(documents)} å€‹å¡Š")
            return documents
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†å¡Šå¤±æ•—: {str(e)}")
            raise
    
    def _extract_technical_terms(self, text: str) -> List[str]:
        """
        æå–æ–‡æœ¬ä¸­çš„æŠ€è¡“è¡“èª
        
        Args:
            text (str): æ–‡æœ¬å…§å®¹
            
        Returns:
            List[str]: æ‰¾åˆ°çš„æŠ€è¡“è¡“èªåˆ—è¡¨
        """
        found_terms = []
        for term in self.technical_terms:
            if term.lower() in text.lower():
                found_terms.append(term)
        return found_terms
    
    def validate_chunks(self, chunks: List[Document]) -> Dict[str, Any]:
        """
        é©—è­‰åˆ†å¡Šå“è³ª
        
        Args:
            chunks (List[Document]): åˆ†å¡Šåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: é©—è­‰çµæœ
        """
        if not chunks:
            return {"quality_score": 0, "issues": ["ç„¡åˆ†å¡Šå…§å®¹"]}
        
        total_chunks = len(chunks)
        total_length = sum(len(chunk.page_content) for chunk in chunks)
        avg_length = total_length / total_chunks if total_chunks > 0 else 0
        
        # çµ±è¨ˆæŠ€è¡“è¡“èª
        technical_terms_count = sum(
            len(chunk.metadata.get("technical_terms", [])) 
            for chunk in chunks
        )
        
        # æª¢æŸ¥åˆ†å¡Šå¤§å°åˆ†å¸ƒ
        size_distribution = {
            "small": sum(1 for chunk in chunks if len(chunk.page_content) < 200),
            "medium": sum(1 for chunk in chunks if 200 <= len(chunk.page_content) <= 800),
            "large": sum(1 for chunk in chunks if len(chunk.page_content) > 800)
        }
        
        # è¨ˆç®—å“è³ªåˆ†æ•¸
        score = 100
        
        # æª¢æŸ¥å¹³å‡é•·åº¦
        if avg_length < 200:
            score -= 20
        elif avg_length > 1000:
            score -= 10
        
        # æª¢æŸ¥æŠ€è¡“è¡“èªè¦†è“‹
        if technical_terms_count < total_chunks * 0.5:
            score -= 15
        
        # æª¢æŸ¥åˆ†å¡Šå¤§å°åˆ†å¸ƒ
        if size_distribution["small"] > total_chunks * 0.3:
            score -= 10
        
        return {
            "quality_score": max(0, score),
            "total_chunks": total_chunks,
            "total_length": total_length,
            "average_length": avg_length,
            "technical_terms_count": technical_terms_count,
            "size_distribution": size_distribution,
            "issues": []
        }
    
    def save_chunks(self, chunks: List[Document], output_path: str) -> None:
        """
        ä¿å­˜åˆ†å¡Šçµæœ
        
        Args:
            chunks (List[Document]): åˆ†å¡Šåˆ—è¡¨
            output_path (str): è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
        """
        try:
            # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_chunks = []
            for chunk in chunks:
                serializable_chunks.append({
                    "content": chunk.page_content,
                    "metadata": chunk.metadata
                })
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_chunks, f, indent=2, ensure_ascii=False)
            
            logger.info(f"åˆ†å¡Šçµæœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†å¡Šçµæœå¤±æ•—: {str(e)}")
            raise

def process_text_file(input_path: str, output_dir: str = None) -> Dict[str, Any]:
    """
    è™•ç†æ–‡æœ¬æ–‡ä»¶çš„å®Œæ•´æµç¨‹
    
    Args:
        input_path (str): è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir (str): è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼‰
        
    Returns:
        Dict[str, Any]: è™•ç†çµæœ
    """
    try:
        # 1. è®€å–æ–‡æœ¬æ–‡ä»¶
        with open(input_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # 2. åˆå§‹åŒ–è™•ç†å™¨
        processor = CREMTextProcessor()
        
        # 3. æ¸…ç†æ–‡æœ¬
        cleaned_text = processor.clean_text(text)
        
        # 4. åˆ†å¡Šæ–‡æœ¬
        chunks = processor.chunk_text(cleaned_text)
        
        # 5. é©—è­‰åˆ†å¡Šå“è³ª
        validation = processor.validate_chunks(chunks)
        
        # 6. ä¿å­˜çµæœï¼ˆå¦‚æœæŒ‡å®šäº†è¼¸å‡ºç›®éŒ„ï¼‰
        if output_dir:
            # ä¿å­˜æ¸…ç†å¾Œçš„æ–‡æœ¬
            cleaned_path = Path(output_dir) / "cleaned_text.txt"
            with open(cleaned_path, 'w', encoding='utf-8') as f:
                f.write(cleaned_text)
            
            # ä¿å­˜åˆ†å¡Šçµæœ
            chunks_path = Path(output_dir) / "text_chunks.json"
            processor.save_chunks(chunks, str(chunks_path))
            
            # ä¿å­˜è™•ç†å ±å‘Š
            report_path = Path(output_dir) / "text_processing_report.json"
            report = {
                "input_file": input_path,
                "cleaned_text_length": len(cleaned_text),
                "chunks_count": len(chunks),
                "validation": validation,
                "processing_timestamp": str(Path().cwd())
            }
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
        
        return {
            "success": True,
            "cleaned_text_length": len(cleaned_text),
            "chunks_count": len(chunks),
            "validation": validation,
            "sample_chunks": [chunk.page_content[:200] + "..." for chunk in chunks[:3]]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    input_path = "data/extracted_text.txt"
    output_dir = "data"
    
    print("=== æ–‡æœ¬æ¸…ç†èˆ‡åˆ†å¡Šæ¸¬è©¦ ===")
    result = process_text_file(input_path, output_dir)
    
    if result["success"]:
        print("âœ… è™•ç†æˆåŠŸ")
        print(f"ğŸ“Š æ¸…ç†å¾Œæ–‡æœ¬é•·åº¦: {result['cleaned_text_length']} å­—ç¬¦")
        print(f"ğŸ“¦ åˆ†å¡Šæ•¸é‡: {result['chunks_count']}")
        print(f"ğŸ“ˆ å“è³ªè©•ä¼°: {result['validation']}")
        print(f"ğŸ“ åˆ†å¡Šæ¨£æœ¬:")
        for i, sample in enumerate(result['sample_chunks'], 1):
            print(f"   å¡Š {i}: {sample}")
        
        # é©—è­‰æ¨™æº–æª¢æŸ¥
        print("\n=== é©—è­‰æ¨™æº–æª¢æŸ¥ ===")
        validation = result['validation']
        
        checks = [
            ("åˆ†å¡Šæ•¸é‡ > 0", validation['total_chunks'] > 0),
            ("å¹³å‡é•·åº¦ 200-1000 å­—ç¬¦", 200 <= validation['average_length'] <= 1000),
            ("å“è³ªåˆ†æ•¸ > 80", validation['quality_score'] > 80),
            ("æŠ€è¡“è¡“èªè¦†è“‹ç‡ > 50%", validation['technical_terms_count'] >= validation['total_chunks'] * 0.5)
        ]
        
        for check_name, passed in checks:
            status = "âœ…" if passed else "âŒ"
            print(f"{status} {check_name}")
        
        all_passed = all(passed for _, passed in checks)
        print(f"\nğŸ¯ æ•´é«”é©—è­‰çµæœ: {'é€šé' if all_passed else 'éœ€è¦æ”¹é€²'}")
        
    else:
        print(f"âŒ è™•ç†å¤±æ•—: {result['error']}") 