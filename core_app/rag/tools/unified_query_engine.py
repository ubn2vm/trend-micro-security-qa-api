"""
çµ±ä¸€æŸ¥è©¢å¼•æ“
æ”¯æ´æ–‡æœ¬å’Œè¡¨æ ¼çš„æ··åˆæŸ¥è©¢ï¼Œæä¾›çµæ§‹åŒ–çš„æœå°‹çµæœ
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from dataclasses import dataclass

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class QueryResult:
    """æŸ¥è©¢çµæœè³‡æ–™çµæ§‹"""
    rank: int
    content_type: str  # 'text' or 'table'
    content: str
    source: str
    confidence_score: float
    metadata: Dict[str, Any]

class UnifiedQueryEngine:
    """çµ±ä¸€æŸ¥è©¢å¼•æ“"""
    
    def __init__(self, vector_dir: str):
        self.vector_dir = Path(vector_dir)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.vector_db = None
        self.query_stats = {
            "total_queries": 0,
            "text_results": 0,
            "table_results": 0,
            "last_query_time": None
        }
    
    def load_vector_db(self) -> bool:
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        faiss_file = self.vector_dir / "index.faiss"
        pkl_file = self.vector_dir / "index.pkl"
        
        if faiss_file.exists() and pkl_file.exists():
            try:
                self.vector_db = FAISS.load_local(
                    str(self.vector_dir), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.info(f"âœ… è¼‰å…¥å‘é‡è³‡æ–™åº«æˆåŠŸ (å‘é‡æ•¸: {self.vector_db.index.ntotal})")
                return True
            except Exception as e:
                logger.error(f"è¼‰å…¥å‘é‡è³‡æ–™åº«å¤±æ•—: {e}")
                return False
        else:
            logger.error("å‘é‡è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨")
            return False
    
    def query(self, question: str, k: int = 5, filter_type: str = "all") -> List[QueryResult]:
        """
        åŸ·è¡Œçµ±ä¸€æŸ¥è©¢
        
        Args:
            question: æŸ¥è©¢å•é¡Œ
            k: è¿”å›çµæœæ•¸é‡
            filter_type: ç¯©é¸é¡å‹ ("all", "text", "table")
            
        Returns:
            æŸ¥è©¢çµæœåˆ—è¡¨
            
        >>> engine = UnifiedQueryEngine("vector_store/crem_faiss_index")
        >>> engine.load_vector_db()
        True
        >>> results = engine.query("é¢¨éšªäº‹ä»¶", k=3)
        >>> len(results) <= 3
        True
        """
        if self.vector_db is None:
            if not self.load_vector_db():
                return []
        
        self.query_stats["total_queries"] += 1
        self.query_stats["last_query_time"] = datetime.now().isoformat()
        
        try:
            # åŸ·è¡Œç›¸ä¼¼æ€§æœå°‹
            docs = self.vector_db.similarity_search_with_score(question, k=k*2)  # å¤šå–ä¸€äº›ä»¥ä¾¿ç¯©é¸
            
            results = []
            text_count = 0
            table_count = 0
            
            for i, (doc, score) in enumerate(docs):
                # åˆ¤æ–·å…§å®¹é¡å‹
                content_type = doc.metadata.get("content_type", "text")
                if content_type == "structured_table":
                    content_type = "table"
                else:
                    content_type = "text"
                
                # ç¯©é¸é¡å‹
                if filter_type != "all" and content_type != filter_type:
                    continue
                
                # å¦‚æœå·²ç¶“æœ‰è¶³å¤ çš„çµæœï¼Œåœæ­¢
                if len(results) >= k:
                    break
                
                # è™•ç†å…§å®¹é¡¯ç¤º
                content = doc.page_content
                if content_type == "table":
                    # è¡¨æ ¼å…§å®¹æ ¼å¼åŒ–
                    content = self._format_table_content(content, doc.metadata)
                    table_count += 1
                else:
                    # æ–‡æœ¬å…§å®¹æˆªå–
                    if len(content) > 300:
                        content = content[:300] + "..."
                    text_count += 1
                
                # å»ºç«‹æŸ¥è©¢çµæœ
                result = QueryResult(
                    rank=len(results) + 1,
                    content_type=content_type,
                    content=content,
                    source=doc.metadata.get("source", "unknown"),
                    confidence_score=1.0 - score,  # FAISSè¿”å›çš„æ˜¯è·é›¢ï¼Œè½‰æ›ç‚ºç›¸ä¼¼åº¦
                    metadata=doc.metadata
                )
                
                results.append(result)
            
            # æ›´æ–°çµ±è¨ˆ
            self.query_stats["text_results"] += text_count
            self.query_stats["table_results"] += table_count
            
            logger.info(f"æŸ¥è©¢å®Œæˆ: '{question}' - æ‰¾åˆ° {len(results)} å€‹çµæœ "
                       f"(æ–‡æœ¬: {text_count}, è¡¨æ ¼: {table_count})")
            
            return results
            
        except Exception as e:
            logger.error(f"æŸ¥è©¢åŸ·è¡Œå¤±æ•—: {e}")
            return []
    
    def _format_table_content(self, content: str, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è¡¨æ ¼å…§å®¹é¡¯ç¤º"""
        table_id = metadata.get("table_id", "unknown")
        table_type = metadata.get("table_type", "general")
        source_page = metadata.get("source_page", "unknown")
        confidence = metadata.get("confidence", 0)
        
        # æå–è¡¨æ ¼çš„é—œéµè³‡è¨Š
        lines = content.split('\n')
        title = lines[0] if lines else "æœªçŸ¥è¡¨æ ¼"
        
        # å»ºç«‹ç°¡æ½”çš„è¡¨æ ¼æ‘˜è¦
        summary_parts = [
            f"ğŸ“Š {title}",
            f"   é¡å‹: {table_type} | é é¢: {source_page} | ä¿¡å¿ƒåº¦: {confidence:.1f}%"
        ]
        
        # æ·»åŠ éƒ¨åˆ†å…§å®¹
        content_lines = [line.strip() for line in lines[1:8] if line.strip()]  # å–å‰7è¡Œ
        if content_lines:
            summary_parts.append("   å…§å®¹é è¦½:")
            for line in content_lines[:5]:  # åªé¡¯ç¤ºå‰5è¡Œ
                if len(line) > 0:
                    summary_parts.append(f"     {line}")
            if len(content_lines) > 5:
                summary_parts.append("     ...")
        
        return '\n'.join(summary_parts)
    
    def search_tables_only(self, question: str, k: int = 5) -> List[QueryResult]:
        """åƒ…æœå°‹è¡¨æ ¼è³‡æ–™"""
        return self.query(question, k=k, filter_type="table")
    
    def search_text_only(self, question: str, k: int = 5) -> List[QueryResult]:
        """åƒ…æœå°‹æ–‡æœ¬è³‡æ–™"""
        return self.query(question, k=k, filter_type="text")
    
    def display_results(self, results: List[QueryResult]) -> None:
        """ç¾è§€åœ°é¡¯ç¤ºæŸ¥è©¢çµæœ"""
        if not results:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçµæœ")
            return
        
        print(f"\nğŸ” æ‰¾åˆ° {len(results)} å€‹çµæœ:\n")
        
        for result in results:
            # çµæœæ¨™é¡Œ
            type_icon = "ğŸ“Š" if result.content_type == "table" else "ğŸ“„"
            print(f"{type_icon} [{result.rank}] {result.content_type.upper()} "
                  f"(ç›¸ä¼¼åº¦: {result.confidence_score:.3f})")
            
            # ä¾†æºè³‡è¨Š
            print(f"   ä¾†æº: {result.source}")
            if result.content_type == "table":
                table_id = result.metadata.get("table_id", "unknown")
                print(f"   è¡¨æ ¼ID: {table_id}")
            
            # å…§å®¹
            print(f"   å…§å®¹:")
            content_lines = result.content.split('\n')
            for line in content_lines:
                if line.strip():
                    print(f"     {line}")
            
            print("-" * 80)
    
    def get_query_stats(self) -> Dict[str, Any]:
        """ç²å–æŸ¥è©¢çµ±è¨ˆè³‡è¨Š"""
        stats = self.query_stats.copy()
        stats["vector_count"] = self.vector_db.index.ntotal if self.vector_db else 0
        return stats


def demo_unified_query():
    """Demoå‡½æ•¸ï¼šçµ±ä¸€æŸ¥è©¢æ¸¬è©¦"""
    engine = UnifiedQueryEngine("vector_store/crem_faiss_index")
    
    if not engine.load_vector_db():
        logger.error("ç„¡æ³•è¼‰å…¥å‘é‡è³‡æ–™åº«")
        return
    
    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        ("é¢¨éšªäº‹ä»¶æœ‰å“ªäº›ï¼Ÿ", "all"),
        ("risky events", "all"), 
        ("è¡¨æ ¼ä¸­çš„çµ±è¨ˆè³‡æ–™", "table"),
        ("å®‰å…¨æ”¿ç­–", "text")
    ]
    
    print("=" * 80)
    print("ğŸš€ çµ±ä¸€æŸ¥è©¢å¼•æ“ Demo")
    print("=" * 80)
    
    for question, filter_type in test_queries:
        print(f"\nğŸ” æŸ¥è©¢: '{question}' (é¡å‹: {filter_type})")
        print("-" * 60)
        
        results = engine.query(question, k=3, filter_type=filter_type)
        engine.display_results(results)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    stats = engine.get_query_stats()
    print("\nğŸ“Š æŸ¥è©¢çµ±è¨ˆ:")
    print(f"   ç¸½æŸ¥è©¢æ¬¡æ•¸: {stats['total_queries']}")
    print(f"   æ–‡æœ¬çµæœæ•¸: {stats['text_results']}")
    print(f"   è¡¨æ ¼çµæœæ•¸: {stats['table_results']}")
    print(f"   å‘é‡è³‡æ–™åº«å¤§å°: {stats['vector_count']} å€‹å‘é‡")
    print(f"   æœ€å¾ŒæŸ¥è©¢æ™‚é–“: {stats['last_query_time']}")


if __name__ == "__main__":
    demo_unified_query() 