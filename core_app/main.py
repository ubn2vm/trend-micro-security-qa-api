import os
import logging
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pathlib import Path

# LangChain ç›¸é—œå°å…¥ - é‡æ–°åŠ å…¥LLMæ”¯æ´
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ RAGæ¨¡çµ„è·¯å¾‘
import sys
rag_dir = Path(__file__).parent / "rag"
sys.path.append(str(rag_dir))

# å°å…¥ç¾æœ‰çš„RAGç³»çµ±
from tools.unified_query_engine import UnifiedQueryEngine

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TrendMicroQASystem:
    """è¶¨å‹¢ç§‘æŠ€è³‡å®‰å ±å‘Šæ™ºèƒ½å•ç­”ç³»çµ±ï¼ˆå®Œæ•´RAG + LLMæ¨¡å¼ï¼‰"""
    
    # å‹•æ…‹ CREM Prompt æ¨¡æ¿
    ENHANCED_CREM_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€å€‹è¶¨å‹¢ç§‘æŠ€è³‡å®‰æŠ€è¡“å°ˆå®¶ï¼Œå°ˆé–€å›ç­”é—œæ–¼ CREM (Cyber Risk Exposure Management) å’Œç¶²è·¯å®‰å…¨çš„å•é¡Œã€‚

ç³»çµ±è³‡è¨Šï¼šæ‚¨æ­£åœ¨ä½¿ç”¨ä¸€å€‹å®Œæ•´çš„çŸ¥è­˜åº«ç³»çµ±ï¼ŒåŸºæ–¼æª¢ç´¢åˆ°çš„ç›¸é—œè³‡æ–™é€²è¡Œåˆ†æã€‚

åŸºæ–¼ä»¥ä¸‹æª¢ç´¢åˆ°çš„ç›¸é—œè³‡æ–™ï¼Œæº–ç¢ºå›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š

=== æª¢ç´¢çµæœ ({result_count}å€‹çµæœ) ===
{context}

=== ç”¨æˆ¶å•é¡Œ ===
{question}

=== å›ç­”æŒ‡å°åŸå‰‡ ===
1. **å……åˆ†åˆ©ç”¨æª¢ç´¢çµæœ**: åŸºæ–¼æä¾›çš„{result_count}å€‹æª¢ç´¢çµæœé€²è¡Œå…¨é¢åˆ†æ
2. **æ•¸æ“šæ´å¯Ÿåˆ¤æ–·**: æª¢ç´¢çµæœä¸­æ˜¯å¦åŒ…å«æ˜ç¢ºçš„æ•¸å­—ã€çµ±è¨ˆæ•¸æ“šã€ç™¾åˆ†æ¯”ã€æ’åã€åœ–è¡¨æ•¸æ“šç­‰å…·é«”é‡åŒ–è³‡è¨Š
3. **å°ˆæ¥­è¡“èªæº–ç¢º**: æ­£ç¢ºä½¿ç”¨ CREMã€CRIã€Trend Vision One ç­‰å°ˆæ¥­è¡“èª
4. **çµæ§‹åŒ–å›ç­”**: æä¾›æ¸…æ™°çš„æ‘˜è¦å’Œè©³ç´°èªªæ˜

=== å›ç­”æ ¼å¼è¦æ±‚ ===
**ğŸ“‹ æ‘˜è¦**
[ç°¡æ½”æ‘˜è¦ï¼Œçªå‡ºæ ¸å¿ƒè¦é»]

**ğŸ” è©³ç´°åˆ†æ**
[åŸºæ–¼æª¢ç´¢çµæœçš„è©³ç´°åˆ†æå’Œè§£é‡‹]

**ğŸ’¡ é—œéµç™¼ç¾**
- [è¦é»1]
- [è¦é»2] 
- [è¦é»3]

**ğŸ“Š æ•¸æ“šæ´å¯Ÿ**
[æ³¨æ„ï¼šåªæœ‰ç•¶æª¢ç´¢çµæœæ˜ç¢ºåŒ…å«æ•¸å­—çµ±è¨ˆã€ç™¾åˆ†æ¯”ã€æ’åã€åœ–è¡¨ã€æ•¸æ“šè¡¨æ ¼ç­‰é‡åŒ–è³‡è¨Šæ™‚ï¼Œæ‰å¯«å‡ºæ­¤éƒ¨åˆ†ã€‚å¦‚æœæª¢ç´¢çµæœä¸»è¦æ˜¯æ¦‚å¿µèªªæ˜ã€åŠŸèƒ½æè¿°ã€å®šç¾©è§£é‡‹ç­‰æ–‡å­—å…§å®¹ï¼Œè«‹ç›´æ¥è·³éæ­¤éƒ¨åˆ†ï¼Œä¸è¦å¯«ã€ŒğŸ“Š æ•¸æ“šæ´å¯Ÿã€æ¨™é¡Œ]

æ³¨æ„ï¼šä¸è¦åœ¨å›ç­”ä¸­åŒ…å«è³‡æ–™ä¾†æºéƒ¨åˆ†ï¼Œç³»çµ±æœƒè‡ªå‹•æ·»åŠ å®Œæ•´çš„ä¾†æºä¿¡æ¯ã€‚

è«‹é–‹å§‹å›ç­”ï¼š
"""

    def __init__(self):
        """åˆå§‹åŒ–å•ç­”ç³»çµ±ï¼ˆæ•´åˆç¾æœ‰RAG + LLMï¼‰"""
        # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        
        config_path = os.path.join(project_root, 'config', 'config.env')
        env_path = os.path.join(project_root, '.env')
        
        load_dotenv(config_path)
        load_dotenv(env_path)
        
        # é©—è­‰ API Keyï¼ˆå¯é¸ï¼‰
        self.llm_available = self._check_api_key()
        
        # åˆå§‹åŒ–ç¾æœ‰RAGç³»çµ±
        self._initialize_rag_system()
        
        # åˆå§‹åŒ– LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.llm_available:
            self._initialize_llm()
    
    def _check_api_key(self) -> bool:
        """æª¢æŸ¥ Google API Keyï¼ˆéå¼·åˆ¶ï¼‰"""
        api_key = os.getenv("GOOGLE_API_KEY")
        
        if not api_key or len(api_key.strip()) < 20:
            logger.warning("âš ï¸ GOOGLE_API_KEY æœªè¨­å®šæˆ–ç„¡æ•ˆï¼Œå°‡ä½¿ç”¨æ ¼å¼åŒ–å›ç­”æ¨¡å¼")
            return False
        
        masked_key = api_key[:4] + "*" * (len(api_key) - 8) + api_key[-4:] if len(api_key) > 8 else "****"
        logger.info(f"âœ… Google API Key å·²è¨­å®š: {masked_key}")
        return True
    
    def _initialize_rag_system(self):
        """åˆå§‹åŒ–RAGç³»çµ±"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–RAGç³»çµ±...")
            
            # ä½¿ç”¨ç¾æœ‰å‘é‡è³‡æ–™åº«
            current_dir = Path(__file__).parent
            vector_dir = current_dir / "rag" / "vector_store" / "crem_faiss_index"
            
            if not vector_dir.exists():
                raise FileNotFoundError(f"RAG å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {vector_dir}")
            
            # åˆå§‹åŒ–çµ±ä¸€æŸ¥è©¢å¼•æ“
            self.rag_engine = UnifiedQueryEngine(str(vector_dir))
            
            if not self.rag_engine.load_vector_db():
                raise Exception("ç„¡æ³•è¼‰å…¥å‘é‡è³‡æ–™åº«")
            
            # å‹•æ…‹ç²å–ç³»çµ±çµ±è¨ˆ
            stats = self.rag_engine.get_query_stats()
            self.vector_count = stats.get('vector_count', 0)
            
            # å‹•æ…‹ç²å–è¡¨æ ¼æ•¸é‡
            self.table_count = self._get_table_count()
            self.estimated_text_count = self.vector_count - self.table_count
            
            logger.info("âœ… RAGç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"ğŸ“Š å‘é‡è³‡æ–™åº«çµ±è¨ˆ: ç¸½è¨ˆ{self.vector_count}å€‹å‘é‡")
            logger.info(f"ğŸ“Š ä¼°ç®—çµ„æˆ: ~{self.estimated_text_count}å€‹æ–‡æœ¬å‘é‡ + {self.table_count}å€‹è¡¨æ ¼å‘é‡")
            
        except Exception as e:
            logger.error(f"RAGç³»çµ±åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    def _get_table_count(self) -> int:
        """å‹•æ…‹ç²å–è¡¨æ ¼æ•¸é‡"""
        try:
            table_texts_file = Path(__file__).parent / "rag" / "data" / "processed" / "table_texts.json"
            if table_texts_file.exists():
                import json
                with open(table_texts_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get('total_tables', 0)
        except Exception as e:
            logger.warning(f"ç„¡æ³•è®€å–è¡¨æ ¼æ•¸é‡: {e}")
        return 0
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLMï¼ˆå¦‚æœAPI Keyå¯ç”¨ï¼‰"""
        try:
            # å¾ç’°å¢ƒè®Šæ•¸å–å¾—æ¨¡å‹è¨­å®š
            model_name = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-lite")
            temperature = float(os.getenv("GEMINI_TEMPERATURE", "0.05"))
            max_tokens = int(os.getenv("GEMINI_MAX_TOKENS", "300"))
            
            logger.info(f"ğŸ¤– åˆå§‹åŒ– Gemini æ¨¡å‹: {model_name}")
            logger.info(f"âš™ï¸ æº«åº¦: {temperature}, æœ€å¤§ Token: {max_tokens}")
            
            self.llm = ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=os.getenv("GOOGLE_API_KEY"),
                temperature=temperature,
                max_output_tokens=max_tokens
            )
            
            # å»ºç«‹ Prompt æ¨¡æ¿
            self.prompt_template = PromptTemplate(
                template=self.ENHANCED_CREM_PROMPT_TEMPLATE,
                input_variables=["context", "question", "result_count"]
            )
            
            logger.info("âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"LLM åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            self.llm_available = False
    
    def ask_question(self, question: str, filter_type: str = "all", k: int = 5) -> Dict[str, Any]:
        """
        å›ç­”å•é¡Œï¼ˆå®Œæ•´RAG + LLMæ¨¡å¼ï¼‰
        
        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            filter_type: æŸ¥è©¢é¡å‹ ("all", "text", "table")  
            k: è¿”å›çµæœæ•¸é‡
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œä¾†æºçš„å­—å…¸
        """
        try:
            logger.info(f"æ”¶åˆ°å•é¡Œ: {question} (é¡å‹: {filter_type})")
            
            # æ­¥é©Ÿ1: ä½¿ç”¨ç¾æœ‰RAGæª¢ç´¢
            detected_filter = self._detect_query_type(question, filter_type)
            results = self.rag_engine.query(
                question=question,
                k=k,
                filter_type=detected_filter
            )
            
            if not results:
                return {
                    "question": question,
                    "answer": f"æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•åœ¨çŸ¥è­˜åº«ï¼ˆåŒ…å«{self.vector_count}å€‹å‘é‡ï¼‰ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚å»ºè­°æ‚¨ï¼š\n1. å˜—è©¦ä½¿ç”¨ä¸åŒçš„é—œéµè©\n2. æå‡ºæ›´å…·é«”çš„å•é¡Œ\n3. æª¢æŸ¥å•é¡Œæ˜¯å¦èˆ‡ç¶²è·¯å®‰å…¨ã€é¢¨éšªç®¡ç†ç›¸é—œ",
                    "sources": [],
                    "citations": [],  # âœ… æ·»åŠ ç©ºçš„citations
                    "status": "no_results",
                    "result_count": 0,
                    "text_results": 0,
                    "table_results": 0,
                    "generation_method": "fallback",
                    "system_type": "TrendMicroQASystem",  # âœ… æ–°å¢ system_type
                    "llm_available": self.llm_available,   # âœ… æ–°å¢ llm_available
                    "vector_db_size": self.vector_count
                }
            
            # æ­¥é©Ÿ2: æ§‹å»ºcontext
            context_parts = []
            text_count = 0
            table_count = 0
            
            for i, result in enumerate(results):
                if result.content_type == "table":
                    table_count += 1
                    context_parts.append(f"[è¡¨æ ¼è³‡æ–™ {i+1}] ä¾†æº: {result.source}\n{result.content}")
                else:
                    text_count += 1
                    context_parts.append(f"[æ–‡æœ¬è³‡æ–™ {i+1}] ä¾†æº: {result.source}\n{result.content}")
            
            context = "\n\n".join(context_parts)
            
            # æ­¥é©Ÿ3: LLMç”Ÿæˆç­”æ¡ˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.llm_available and hasattr(self, 'llm'):
                try:
                    prompt = self.prompt_template.format(
                        context=context, 
                        question=question,
                        result_count=len(results)
                    )
                    response = self.llm.invoke(prompt)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    generation_method = "llm_generated"
                    
                    logger.info("âœ… LLM ç­”æ¡ˆç”ŸæˆæˆåŠŸ")
                    
                except Exception as llm_error:
                    logger.error(f"LLM ç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°æ ¼å¼åŒ–ç­”æ¡ˆ: {llm_error}")
                    answer = self._generate_structured_answer(results, question)
                    generation_method = "fallback_structured"
            else:
                # ä½¿ç”¨çµæ§‹åŒ–æ ¼å¼åŒ–ç­”æ¡ˆ
                answer = self._generate_structured_answer(results, question)
                generation_method = "structured_formatted"
            
            # æ­¥é©Ÿ4: æå–ä¾†æºè³‡è¨Šå’Œå¼•ç”¨å…§å®¹
            sources = []
            citations = []

            logger.info(f"ğŸ” Debug: æº–å‚™è™•ç† {len(results)} å€‹æª¢ç´¢çµæœ")

            for i, result in enumerate(results[:3]):
                # ç¢ºä¿confidence_scoreæ˜¯Python floaté¡å‹
                confidence_value = float(result.confidence_score) if hasattr(result.confidence_score, 'item') else float(result.confidence_score)
                
                source_info = f"[{result.content_type.upper()}] {result.source} (ä¿¡å¿ƒåº¦: {confidence_value:.2f})"
                sources.append(source_info)
                
                # æ·»åŠ å¼•ç”¨å…§å®¹ - æ‰€æœ‰å€¼éƒ½è½‰æ›ç‚ºPythonåŸç”Ÿé¡å‹
                citation = {
                    "rank": int(i + 1),
                    "source": str(result.source),
                    "content_type": str(result.content_type),
                    "content": str(result.content),
                    "confidence": confidence_value  # Python float
                }
                citations.append(citation)
                logger.info(f"ğŸ” Debug: æ·»åŠ citation {i+1}: {result.source} - å…§å®¹é•·åº¦: {len(result.content)} - ä¿¡å¿ƒåº¦é¡å‹: {type(confidence_value)}")

            logger.info(f"ğŸ” Debug: ç¸½å…±å‰µå»ºäº† {len(citations)} å€‹citations")

            response = {
                "question": question,
                "answer": answer,
                "sources": sources,
                "citations": citations,  # âœ… ç¢ºä¿åŒ…å«citations
                "status": "success",
                "result_count": len(results),
                "text_results": text_count,
                "table_results": table_count,
                "filter_type": detected_filter,
                "generation_method": generation_method,
                "system_type": "TrendMicroQASystem",
                "llm_available": self.llm_available,
                "vector_db_size": self.vector_count
            }
            
            logger.info(f"ğŸ” Debug: éŸ¿æ‡‰ä¸­åŒ…å« {len(response.get('citations', []))} å€‹citations")
            logger.info(f"å•é¡Œå›ç­”å®Œæˆ: æ‰¾åˆ°{len(results)}å€‹çµæœ (æ–‡æœ¬:{text_count}, è¡¨æ ¼:{table_count})")
            return response
            
        except Exception as e:
            logger.error(f"å›ç­”å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "sources": [],
                "citations": [],  # âœ… æ·»åŠ ç©ºçš„citations
                "status": "error",
                "result_count": 0,
                "text_results": 0,
                "table_results": 0,
                "generation_method": "error",
                "system_type": "TrendMicroQASystem",  # âœ… æ–°å¢ system_type
                "llm_available": self.llm_available,   # âœ… æ–°å¢ llm_available
                "vector_db_size": getattr(self, 'vector_count', 0)
            }
    
    def _detect_query_type(self, question: str, default_filter: str) -> str:
        """æ™ºèƒ½æª¢æ¸¬æŸ¥è©¢é¡å‹ - å„ªåŒ–ç‰ˆ"""
        if default_filter != "all":
            return default_filter
            
        question_lower = question.lower()
        
        # è¡¨æ ¼ç›¸é—œé—œéµè© - æ•¸æ“šå°å‘
        table_keywords = [
            # æ’è¡Œæ¦œé¡
            "å‰10", "top 10", "å‰å", "å‰5", "top 5", "æ’è¡Œ", "æ’å", "ranking",
            # çµ±è¨ˆæ•¸æ“šé¡
            "çµ±è¨ˆ", "æ•¸æ“š", "è¡¨æ ¼", "statistics", "data", "table", "chart",
            # æ¯”è¼ƒåˆ†æé¡
            "æ¯”è¼ƒ", "å°æ¯”", "åˆ†æ", "comparison", "analysis", "versus", "vs",
            # åˆ—è¡¨é¡
            "list", "æ¸…å–®", "åˆ—è¡¨", "é …ç›®",
            # ç‰¹å®šé¢¨éšªäº‹ä»¶
            "risky events", "é¢¨éšªäº‹ä»¶", "å¨è„…äº‹ä»¶", "security incidents",
            # æ•¸é‡è©
            "å¤šå°‘", "å¹¾å€‹", "how many", "count"
        ]
        
        # æ–‡æœ¬ç›¸é—œé—œéµè© - æ¦‚å¿µå’Œèªªæ˜å°å‘
        text_keywords = [
            # æ”¿ç­–æŒ‡å°é¡
            "æ”¿ç­–", "policy", "policies", "è¦ç¯„", "æº–å‰‡", "guidelines",
            # å»ºè­°è«®è©¢é¡  
            "å»ºè­°", "recommendation", "recommendations", "suggest", "advice",
            # ç­–ç•¥è¦åŠƒé¡
            "ç­–ç•¥", "æˆ°ç•¥", "strategy", "strategies", "approach", "framework",
            # æ–¹æ³•æ­¥é©Ÿé¡
            "æ–¹æ³•", "æ­¥é©Ÿ", "æµç¨‹", "method", "methods", "process", "procedure",
            # è©¢å•è§£é‡‹é¡
            "å¦‚ä½•", "æ€éº¼", "æ€æ¨£", "how to", "how can", "how do", "how does",
            # å®šç¾©æ¦‚å¿µé¡
            "ä»€éº¼æ˜¯", "ä»€éº¼å«", "å®šç¾©", "what is", "what are", "define", "definition",
            # è§£é‡‹èªªæ˜é¡
            "è§£é‡‹", "èªªæ˜", "ä»‹ç´¹", "explain", "explanation", "describe", "overview",
            # åŸå› åˆ†æé¡
            "ç‚ºä»€éº¼", "åŸå› ", "why", "because", "reason", "cause",
            # åŠŸèƒ½ä½œç”¨é¡
            "åŠŸèƒ½", "ä½œç”¨", "ç”¨é€”", "function", "purpose", "benefit", "advantage",
            # å¯¦æ–½åŸ·è¡Œé¡
            "å¯¦æ–½", "åŸ·è¡Œ", "éƒ¨ç½²", "implement", "deploy", "execute",
            # æœ€ä½³å¯¦è¸é¡
            "æœ€ä½³", "æœ€å¥½", "å„ªåŒ–", "best", "optimal", "improve", "enhancement"
        ]
        
        # æ··åˆæŸ¥è©¢é—œéµè© - å¯èƒ½éœ€è¦è¡¨æ ¼+æ–‡æœ¬çµåˆ
        hybrid_keywords = [
            "è¶¨å‹¢", "ç¾ç‹€", "ç‹€æ³", "æƒ…æ³", "trend", "current", "situation", "status",
            "è©•ä¼°", "åˆ†æå ±å‘Š", "assessment", "evaluation", "report"
        ]
        
        # æª¢æŸ¥è¡¨æ ¼é—œéµè©ï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
        if any(keyword in question_lower for keyword in table_keywords):
            return "table"
            
        # æª¢æŸ¥æ–‡æœ¬é—œéµè©
        if any(keyword in question_lower for keyword in text_keywords):
            return "text"
            
        # æª¢æŸ¥æ··åˆé—œéµè©
        if any(keyword in question_lower for keyword in hybrid_keywords):
            return "all"
            
        # é è¨­æ··åˆæŸ¥è©¢
        return "all"
    
    def _generate_structured_answer(self, results: List, question: str) -> str:
        """ç”Ÿæˆçµæ§‹åŒ–ç­”æ¡ˆï¼ˆç•¶LLMä¸å¯ç”¨æ™‚ï¼‰"""
        if not results:
            return "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•æ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚"
        
        best_result = results[0]
        
        # æ ¹æ“šçµæœé¡å‹ç”Ÿæˆä¸åŒæ ¼å¼çš„ç­”æ¡ˆ
        if best_result.content_type == "table":
            if any(keyword in question.lower() for keyword in ["å‰10", "top 10", "æ’è¡Œ", "å‰å"]):
                answer = f"ğŸ“Š **æ ¹æ“šçŸ¥è­˜åº«çš„çµ±è¨ˆè³‡æ–™ï¼š**\n\n{best_result.content}"
                
                if len(results) > 1:
                    answer += f"\n\n**ç›¸é—œè£œå……è³‡è¨Šï¼š**\n{results[1].content[:200]}..."
                
                # âœ… ç§»é™¤è³‡æ–™ä¾†æºéƒ¨åˆ†
                return answer
        
        # ä¸€èˆ¬çµæ§‹åŒ–å›ç­”æ ¼å¼
        answer = f"**ğŸ“‹ æ‘˜è¦**\nåŸºæ–¼çŸ¥è­˜åº«æª¢ç´¢çµæœï¼š\n\n{best_result.content[:400]}"
        
        if len(best_result.content) > 400:
            answer += "..."
        
        # æ·»åŠ ç›¸é—œè³‡è¨Š
        if len(results) > 1:
            second_result = results[1]
            answer += f"\n\n**ğŸ” ç›¸é—œè³‡è¨Š**\n{second_result.content[:200]}"
            if len(second_result.content) > 200:
                answer += "..."
        
        # âœ… ç§»é™¤è³‡æ–™ä¾†æºéƒ¨åˆ†ï¼Œç³»çµ±æœƒè‡ªå‹•æ·»åŠ 
        return answer
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Š"""
        if hasattr(self, 'rag_engine'):
            stats = self.rag_engine.get_query_stats()
            current_vector_count = stats.get('vector_count', 0)
            
            return {
                "system_type": "TrendMicroQASystem",  # âœ… ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„ç³»çµ±é¡å‹
                "system_description": f"å®Œæ•´RAG+LLMç³»çµ± ({current_vector_count}å‘é‡)",  # æè¿°ç§»åˆ°æ–°æ¬„ä½
                "vector_count": current_vector_count,
                "estimated_text_vectors": self.estimated_text_count,
                "table_vectors": self.table_count,
                "total_queries": stats.get('total_queries', 0),
                "text_results": stats.get('text_results', 0),
                "table_results": stats.get('table_results', 0),
                "last_query_time": stats.get('last_query_time'),
                "llm_available": self.llm_available,
                "llm_model": os.getenv("GEMINI_MODEL", "gemini-2.0-flash-lite") if self.llm_available else "N/A",
                "capabilities": [
                    f"{current_vector_count}å€‹å‘é‡å®Œæ•´æª¢ç´¢ (~{self.estimated_text_count}æ–‡æœ¬ + {self.table_count}è¡¨æ ¼)",
                    "Gemini LLMè‡ªç„¶èªè¨€ç”Ÿæˆ" if self.llm_available else "çµæ§‹åŒ–æ ¼å¼å›ç­”",
                    "å¤šèªè¨€æ”¯æ´(ä¸­æ–‡/è‹±æ–‡)",
                    "æ™ºèƒ½æŸ¥è©¢é¡å‹æª¢æ¸¬",
                    "è¡¨æ ¼å’Œæ–‡æœ¬æ··åˆæŸ¥è©¢",
                    "ç½®ä¿¡åº¦è©•ä¼°",
                    "ä¾†æºè¿½è¹¤"
                ]
            }
        else:
            return {
                "system_type": "TrendMicroQASystem",
                "system_description": "ç³»çµ±æœªåˆå§‹åŒ–",
                "vector_count": 0,
                "estimated_text_vectors": 0,
                "table_vectors": 0,
                "total_queries": 0,
                "text_results": 0,
                "table_results": 0,
                "last_query_time": None,
                "llm_available": False,
                "llm_model": "N/A",
                "capabilities": []
            }
    
    def test_system_integrity(self) -> Dict[str, Any]:
        """æ¸¬è©¦ç³»çµ±å®Œæ•´æ€§"""
        try:
            # æ¸¬è©¦RAGæª¢ç´¢
            test_results = self.rag_engine.query("CREM", k=1)
            rag_status = "success" if test_results else "no_results"
            
            # æ¸¬è©¦LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰
            llm_status = "not_available"
            if self.llm_available and hasattr(self, 'llm'):
                try:
                    test_response = self.llm.invoke("ä»€éº¼æ˜¯CREMï¼Ÿè«‹ç°¡çŸ­å›ç­”ã€‚")
                    llm_status = "success" if test_response else "failed"
                except:
                    llm_status = "failed"
            
            stats = self.rag_engine.get_query_stats()
            current_vector_count = stats.get('vector_count', 0)
            
            return {
                "overall_status": "healthy",
                "rag_status": rag_status,
                "llm_status": llm_status,
                "vector_count": current_vector_count,
                "table_count": self.table_count,
                "estimated_text_count": self.estimated_text_count,
                "message": f"ç³»çµ±é‹è¡Œæ­£å¸¸ï¼ŒåŒ…å«{current_vector_count}å€‹å‘é‡"
            }
        except Exception as e:
            return {
                "overall_status": "error",
                "message": f"ç³»çµ±æ¸¬è©¦å¤±æ•—: {str(e)}"
            }

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦å®Œæ•´ç³»çµ±"""
    try:
        # å»ºç«‹å•ç­”ç³»çµ±
        qa_system = TrendMicroQASystem()
        
        # æ¸¬è©¦ç³»çµ±å®Œæ•´æ€§
        integrity_test = qa_system.test_system_integrity()
        logger.info(f"ç³»çµ±å®Œæ•´æ€§æ¸¬è©¦: {integrity_test['overall_status']}")
        logger.info(f"å‘é‡çµ±è¨ˆ: ç¸½è¨ˆ{integrity_test.get('vector_count', 'unknown')}å€‹")
        logger.info(f"çµ„æˆ: ~{integrity_test.get('estimated_text_count', 'unknown')}æ–‡æœ¬ + {integrity_test.get('table_count', 'unknown')}è¡¨æ ¼")
        
        # æ¸¬è©¦å•é¡Œ
        test_questions = [
            ("å‰10å¤§é¢¨éšªäº‹ä»¶æœ‰å“ªäº›ï¼Ÿè«‹è©³ç´°åˆ†æ", "all"),
            ("ä»€éº¼æ˜¯CREMï¼Ÿå®ƒå¦‚ä½•å¹«åŠ©ä¼æ¥­ç®¡ç†ç¶²è·¯é¢¨éšªï¼Ÿ", "text"),
            ("çµ±è¨ˆè³‡æ–™é¡¯ç¤ºçš„ä¸»è¦å¨è„…æœ‰å“ªäº›ï¼Ÿ", "table"),
            ("risky cloud app accessç›¸é—œçš„è¡¨æ ¼æ•¸æ“š", "table"),
            ("ä¼æ¥­å®‰å…¨æ”¿ç­–çš„æœ€ä½³å¯¦è¸å»ºè­°", "text")
        ]
        
        logger.info("=== è¶¨å‹¢ç§‘æŠ€å®Œæ•´RAG+LLMç³»çµ±æ¸¬è©¦ ===")
        
        for question, filter_type in test_questions:
            logger.info(f"å•é¡Œ: {question} (é¡å‹: {filter_type})")
            result = qa_system.ask_question(question, filter_type=filter_type)
            logger.info(f"ç‹€æ…‹: {result['status']}")
            logger.info(f"ç”Ÿæˆæ–¹å¼: {result.get('generation_method', 'unknown')}")
            logger.info(f"LLMå¯ç”¨: {result.get('llm_available', False)}")
            logger.info(f"ç­”æ¡ˆé è¦½: {result['answer'][:150]}...")
            if result['sources']:
                logger.info(f"ä¾†æº: {result['sources'][0]}")
            logger.info(f"çµæœçµ±è¨ˆ: {result.get('result_count', 0)}å€‹çµæœ "
                       f"(æ–‡æœ¬:{result.get('text_results', 0)}, è¡¨æ ¼:{result.get('table_results', 0)})")
            logger.info("-" * 80)
            
        # é¡¯ç¤ºç³»çµ±çµ±è¨ˆ
        stats = qa_system.get_system_stats()
        logger.info("=== ç³»çµ±çµ±è¨ˆ ===")
        for key, value in stats.items():
            logger.info(f"{key}: {value}")
            
    except Exception as e:
        logger.error(f"ç³»çµ±åˆå§‹åŒ–æˆ–æ¸¬è©¦å¤±æ•—: {str(e)}")
        logger.error("å»ºè­°æª¢æŸ¥ï¼š")
        logger.error("1. å‘é‡è³‡æ–™åº«æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´")
        logger.error("2. GOOGLE_API_KEY è¨­å®šï¼ˆå¯é¸ï¼Œä¸å½±éŸ¿åŸºæœ¬åŠŸèƒ½ï¼‰")
        logger.error("3. ç¶²è·¯é€£æ¥ï¼ˆåƒ…LLMåŠŸèƒ½éœ€è¦ï¼‰")

if __name__ == "__main__":
    main() 